{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GTI771 - Apprentissage machine avancé\n",
    "\n",
    "### Created: Thiago M. Paixão <br> Created/Revised: Alessandro L. Koerich <br> Ver 1.0 <br> December 2020¶"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NB0 - Generalization with Linear/Polinomial Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook addresses the regression task using a [Bayesian approach](https://en.wikipedia.org/wiki/Bayesian_linear_regression), which is also a supervised learning task. The focus is shown the challenge of generalizing a model from a set of data points.\n",
    "\n",
    "The notebook is divided into four parts:\n",
    "\n",
    "* Setup\n",
    "* Data generation\n",
    "* Regression\n",
    "    * Linear regression\n",
    "    * $n$-degree polinomal regression\n",
    "* MSE analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data generation\n",
    "\n",
    "We need to generate some data to play with. So, we need to choose a base function (true function) where training and test data will be derived from. The functions has the form $r = f(x)$, where $r$ is the output (label) and $x$ is the input (features). Example of functions: \n",
    "\n",
    "* $r = x + 2$\n",
    "* $r = x^2 + 2x + 4$\n",
    "* $r = \\sin(2\\pi x)$\n",
    "* ...\n",
    "\n",
    "In our demonstration, we chose $r = \\cos(6\\pi x)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "f_true = lambda X: np.cos(6 * np.pi * X)\n",
    "# f_true = lambda X: 15*(X-0.5)*(X-0.5) - 1.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we generate training and test data based on the above function. We assume that features ($x$) are real values sampled from the interval $[0, 1]$ and that the corresponding label is given by the true function plus some random noise, i.e., $r_i = f(x_i) + \\delta_i$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# seed the experiment\n",
    "np.random.seed(0)\n",
    "\n",
    "n_samples_train = n_samples_test = 60\n",
    "\n",
    "# random data points\n",
    "X_train = np.sort(np.random.rand(n_samples_train))\n",
    "X_test  = np.sort(np.random.rand(n_samples_test))\n",
    "\n",
    "# corresponding labels\n",
    "delta_train = np.random.randn(n_samples_train) * 0.2\n",
    "delta_test = np.random.randn(n_samples_test) * 0.2\n",
    "\n",
    "r_train = f_true(X_train) + delta_train\n",
    "r_test = f_true(X_test) + delta_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the data points and also the true function, i.e., the function used to generate the data points:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# chart setup\n",
    "fig, (ax1, ax2) = plt.subplots(ncols=2, sharey=True, figsize=(16, 4))\n",
    "\n",
    "X_dummy = np.linspace(0, 1., 100)\n",
    "\n",
    "ax1.plot(X_dummy, f_true(X_dummy), label='True function', color='gray')\n",
    "\n",
    "ax1.scatter(X_train, r_train, edgecolor='b', s=20, label='Training samples')\n",
    "ax1.scatter(X_test, r_test, edgecolor='y', s=40, label='Test samples')\n",
    "ax1.set_ylabel('r')\n",
    "ax1.set_xlabel('x')\n",
    "ax1.set_title('Data points (with the true function)')\n",
    "ax1.set_xlim((0, 1))\n",
    "ax1.set_ylim((-2, 2))\n",
    "\n",
    "ax2.scatter(X_train, r_train, edgecolor='b', s=20, label='Training samples')\n",
    "ax2.scatter(X_test, r_test, edgecolor='y', s=40, label='Test samples')\n",
    "ax2.set_xlabel('x')\n",
    "ax2.set_title('Data points (without the true function)')\n",
    "ax2.set_xlim((0, 1))\n",
    "ax2.set_ylim((-1.5, 1.5))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression\n",
    "\n",
    "Some error metrics can be used to evaluate the regression quality, such as\n",
    "\n",
    "* Mean Square Error ([MSE](https://scikit-learn.org/stable/modules/model_evaluation.html#mean-squared-error))\n",
    "* Mean Absolute Value ([MAE](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_absolute_error.html))\n",
    "\n",
    "We defined both in the next cell. Although we use only MSE in the examples, you can replace it - as exercise - by MAE and analyze the behaviour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Compute the MSE and MAE metric for Regression\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error, make_scorer\n",
    "\n",
    "MSE = lambda y_true, y_pred: mean_squared_error(y_true, y_pred)\n",
    "MAE = lambda y_true, y_pred: mean_absolute_error(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "plt.setp(ax, xticks=(), yticks=()) # disable ticks\n",
    "\n",
    "# linear regression\n",
    "degrees = 1\n",
    "linear_regression = LinearRegression()\n",
    "linear_regression.fit(X_train[:, np.newaxis], r_train)\n",
    "\n",
    "# evaluate the models using the X_test samples\n",
    "Y_pred_test = linear_regression.predict(X_test[:, np.newaxis])\n",
    "Y_pred_train = linear_regression.predict(X_train[:, np.newaxis])\n",
    "\n",
    "ax.plot(X_test, linear_regression.predict(X_test[:, np.newaxis]), label='Model', color='g')\n",
    "ax.plot(X_train, f_true(X_train), label='True function', color='gray')\n",
    "\n",
    "ax.scatter(X_train, r_train, edgecolor='b', s=20, label='Train samples')\n",
    "ax.scatter(X_test, r_test, edgecolor='y', s=40, label='Test samples')\n",
    "\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel('r')\n",
    "ax.set_xlim((0, 1))\n",
    "ax.set_ylim((-2, 2))\n",
    "ax.legend(loc='best')\n",
    "ax.set_title(\"Degree {}\\nMSE on test set = {:.5f}\\nMSE on training set = {:.5f}\".format(degrees, MSE(r_test,Y_pred_test), MSE(r_train,Y_pred_train)))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $n$-D polynomial regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "def plot_regression(degree):\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    plt.setp(ax, xticks=(), yticks=()) # disable ticks\n",
    "\n",
    "    ax.scatter(X_train, r_train, edgecolor='b', s=20, label='Train samples')\n",
    "    ax.scatter(X_test, r_test, edgecolor='y', s=40, label='Test samples')\n",
    "\n",
    "    polynomial_features = PolynomialFeatures(degree=degree, include_bias=False)\n",
    "    linear_regression = LinearRegression()\n",
    "    pipeline = Pipeline([\n",
    "        ('polynomial_features', polynomial_features),\n",
    "        ('linear_regression', linear_regression)\n",
    "    ])\n",
    "    pipeline.fit(X_train[:, np.newaxis], r_train)\n",
    "\n",
    "    # evaluate the models using the X_test samples\n",
    "    Y_pred_test = pipeline.predict(X_test[:, np.newaxis])\n",
    "    Y_pred_train = pipeline.predict(X_train[:, np.newaxis])\n",
    "\n",
    "    ax.plot(X_test, pipeline.predict(X_test[:, np.newaxis]), label='Model (degree {})'.format(degree), color='g')\n",
    "    ax.plot(X_train, f_true(X_train), label='True function', color='gray')\n",
    "    ax.set_xlabel('x')\n",
    "    ax.set_ylabel('r')\n",
    "    ax.set_xlim((0, 1))\n",
    "    ax.set_ylim((-1.5, 1.5))\n",
    "    ax.legend(loc='best', ncol=2)\n",
    "    ax.set_title('Degree {}\\nMSE on test set = {:.5f}\\nMSE on training set = {:.5f}'.format(degree, MSE(r_test, Y_pred_test), MSE(r_train, Y_pred_train)))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "degrees = [1, 2, 3, 5, 9, 30]\n",
    "for degree in degrees:\n",
    "    plot_regression(degree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MSE analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def plot_error(degrees):\n",
    "    fig, ax = plt.subplots(figsize=(16, 4))\n",
    "    mse_train = []\n",
    "    mse_test = []\n",
    "    for degree in degrees:\n",
    "        polynomial_features = PolynomialFeatures(degree=degree, include_bias=False)\n",
    "        linear_regression = LinearRegression()\n",
    "        pipeline = Pipeline([\n",
    "            ('polynomial_features', polynomial_features),\n",
    "            ('linear_regression', linear_regression)\n",
    "        ])\n",
    "        pipeline.fit(X_train[:, np.newaxis], r_train)\n",
    "\n",
    "        # evaluate the models using the X_test samples\n",
    "        Y_pred_test = pipeline.predict(X_test[:, np.newaxis])\n",
    "        Y_pred_train = pipeline.predict(X_train[:, np.newaxis])\n",
    "        \n",
    "        mse_train.append(MSE(r_train, Y_pred_train))\n",
    "        mse_test.append(MSE(r_test, Y_pred_test))\n",
    "\n",
    "    ax.plot(degrees, mse_train, label='MSE (Train)')\n",
    "    ax.plot(degrees, mse_test, label='MSE (Test)')\n",
    "    ax.set_xlabel('degree')\n",
    "    ax.set_xticks(degrees)\n",
    "    ax.legend(loc='best')\n",
    "    ax.set_title('MSE error')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Degree $\\in [1, 15]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "degrees = list(range(1, 16))\n",
    "plot_error(degrees)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Degree $\\in [16, 30]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "degrees = list(range(16, 31))\n",
    "plot_error(degrees)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
